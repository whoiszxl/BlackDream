### Kafka工作流程
Kafka以`topic`进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。

topic是逻辑上的概念，partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。


### 生产者Producer流程解析

#### 工作原理
1. Kafka发送的消息会包装成ProducerRecord对象，其包含了主题和内容，通过序列化后在网络上发送出去。
2. 可以指定partiton分区，若未指定，则通过key的hash或者随机值计算得出。（round-robin算法）
3. 然后记录会被添加到一个批次里，有独立线程把这个批次发送到broker中。
4. 写入broker会返回一个RecordMetaData对象，写入失败会返回错误并重试发送，知道超过重试次数。

#### 数据可靠性
producer发送数据到broker的topic的partition中后，需要向producer发送ack（`acknowledgement确认收到`）才能确定是有效发送。

kafka的leader节点维护了一个`in-sync replica set (ISR)`，是保存了和leader数据同步的follower集合，如果follower在`replica.lag.time.max.ms`时间范围内未同步leader数据，则被踢出。

#### ack的配置
1. 0：不等待ack，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；
2. 1：等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据；
3. -1（all）：等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。


### 消费者consumer流程解析

#### 简单分析
1. consumer通过pull的方式从broker中读取数据，pull可以根据consumer的消费能力以适当的速率消费消息，不足是在没数据时会死循环，不过timeout时间可以避免。
2. Kafka用`两种分配策略roundrobin和range`来分配哪个partition由哪个consumer来消费
3. consumer默认将offset保存在Kafka一个内置的topic中，该topic为`__consumer_offsets`
4. 因为是顺序写磁盘，所以效率很高。
5. consumer可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。所以offset的维护是Consumer消费数据是必须考虑的问题。可以通过代码手动提交`（consumer.commitSync();）`