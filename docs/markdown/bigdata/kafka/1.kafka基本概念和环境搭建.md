### Kafka基本概念
> Kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。`Kafka是一种高吞吐量的分布式发布订阅消息系统`，它可以处理消费者在网站中的所有动作流数据。

### 队列模式
1. 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）
2. 发布/订阅模式（一对多，消费者消费数据之后不会清除消息）

### 基础架构
1. Producer：消息生产者，向kafka broker发消息的客户端；
2. Consumer：消息消费者，向kafka broker取消息的客户端；
3. Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。
4. Broker：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。
5. Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic；
6. Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；
7. Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。
8. leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。
9. follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower。


### Kafka环境搭建
搭建结构
| Hadoop001     | Hadoop002           | Hadoop003        |
| ------------- | ------------------- | -----------------|
| zookeeper     | zookeeper           | zookeeper        |
| kafka         |                     |                  |

下载地址：http://kafka.apache.org/downloads.html

1. 下载安装包：`https://archive.apache.org/dist/kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz`
2. 解压包：`tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/ && cd /opt/module && mv mv kafka_2.11-0.11.0.0/ kafka`
3. 创建日志文件夹：`cd /opt/module/kafka && mkdir logs`
4. 修改配置文件：`vi ./conf/server.properties`
```conf
#broker的全局唯一编号，不能重复
broker.id=0
#删除topic功能使能
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的现成数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志存放的路径	
log.dirs=/opt/module/kafka/logs
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#配置连接Zookeeper集群地址,一般是集群地址，测试用单台。
zookeeper.connect=hadoop001:2181
```

5. 配置环境变量：`sudo vi /etc/profile && source /etc/profile`
```conf
#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```

6. 分发包：`xsync /opt/module/kafka`
7. 修改hadoop002,hadoop003机器的`/opt/module/kafka/config/server.properties`中`broker.id=1、broker.id=2`
8. 每台机器依次启动：`bin/kafka-server-start.sh -daemon config/server.properties`
9. 关闭服务：`bin/kafka-server-stop.sh stop`
10. kafka批量启动脚本：
```sh
for i in `cat /opt/module/hadoop-2.7.2/etc/hadoop/slaves`
do
echo "========== $i ==========" 
ssh $i 'source /etc/profile&&/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties'
echo $?
done
```

### 基础命令行操作
1. 查看所有topic：`bin/kafka-topics.sh --zookeeper hadoop001:2181 --list`
2. 创建topic：`bin/kafka-topics.sh --zookeeper hadoop001:2181 --create --replication-factor 3 --partitions 1 --topic first`
3. 删除topic：`bin/kafka-topics.sh --zookeeper hadoop001:2181 --delete --topic first`
4. 发送消息：`bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic first`
5. 消费消息：`bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --from-beginning --topic first`
6. 查看某个Topic的详情：`bin/kafka-topics.sh --zookeeper hadoop001:2181 --describe --topic first`
7. 修改分区数：`bin/kafka-topics.sh --zookeeper hadoop001:2181 --alter --topic first --partitions 6`