### 启动
1. 创建一个后台窗口并启动hiveserver2：`screen -S hiveserver2 && ./bin/hiveserver2`
2. 启动beeline执行命令：`./bin/beeline`

### Hive常用命令
1. 获取帮助：`./bin/hive -help`
2. 不使用命令行执行SQL：`./bin/hive -e "select * from user"`
3. 执行SQL文件：`./bin/hive -f /opt/user.sql`
4. 退出hive命令行：`exit; or quit;`
5. 在Hive命令行中执行HDFS命令：`dfs -ls /`
6. 在Hive命令行中执行本地命令：`! ls /opt/`
7. 查看Hive历史输入：`tail -f ~/.hivehistory`
8. 查看Hive配置：`set;`(输出太多，慎执行)

### 数据库相关操作
1. 创建数据库
```sql
CREATE DATABASE [IF NOT EXISTS] database_name --创建数据库，如果不存在
[COMMENT database_comment] --数据库注释
[LOCATION hdfs_path] --存储在 HDFS 上的位置
[WITH DBPROPERTIES (property_name=property_value, ...)]; --指定额外属性
```

2. 查看所有数据库：`show database;`
3. 选择操作数据库：`use database_name;`
4. 查看数据库详情：`desc database [extended] database_name`,extended标识是否显示额外属性，在第五步修改的数据库就能查询到添加的信息
5. 修改数据库：`alter database db_hive set dbproperties('createtime'='19941020');`
6. 删除空数据库：`drop database if exists db_hive2;`
7. 删除非空数据库：`drop database db_hive2 cascade;`


### 表相关操作
1. 创建表
官方文档地址：`https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable`
```sql

CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- 创建临时或者外部表，如果表未存在的话
  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] -- 列名列类型
  [COMMENT table_comment] -- 表注释
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] -- 表分区策略
  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] -- 表分桶策略
  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]
     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)
     [STORED AS DIRECTORIES]
  [
   [ROW FORMAT row_format] 
   [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)
  ]
  [LOCATION hdfs_path]
  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)
  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)
```

#### 内部表
1. 创建内部表
> 内部表是Hive或多或少控制生命周期，删除一个内部表时，Hive也会删除表中数据，不适合和其他工具共享数据。
```sql
create table if not exists xl_user (
    user_id int comment '用户ID',
    user_name string comment '用户名'
)
row format delimited fields terminated by '\t'
stored as textfile
location '/user/hive/warehouse/xl_user.db';
```

2. 根据查询结果创建表：`create table if not exists xl_user2 as select id, name from xl_user1;`
3. 根据已存在表的结构创建表：`create table if not exists xl_user2 like xl_user1;`
4. 查询表的类型结构：`desc formatted xl_user;`

#### 外部表
> 理论：外部表Hive不认为表完全拥有这份数据，`删除外部表不会删除掉这份数据`，但是描述表的元数据信息会被删除。
> 应用：通过外部表进行大量的统计分析，中间表和结果表使用内部表存储，数据通过`insert into xxx select`的方式进入内部表。
```sql
create external table student_external(
    id int, 
    name string
) 
row format delimited fields terminated by '\t' 
location '/student';
```

#### 分区表
> Hive分区表就是分目录表，把大数据集按业务分割成多个小数据集，查询时通过指定分区能提升大量性能。

1. 创建分区表
```sql
create table student_partition(
    user_id int comment '用户ID',
    user_name string comment '用户名'
)
partitioned by (user_address string)
row format delimited fields terminated by '\t';
```

2. 加载数据到分区表
```sql
load data local inpath '/opt/module/datas/student0.txt' into table student_partition partition(user_address='JiangXi');
load data local inpath '/opt/module/datas/student1.txt' into table student_partition partition(user_address='Hunan');
load data local inpath '/opt/module/datas/student2.txt' into table tudent_partition partition(user_address='Guangdong');
```

3. 查询分区表中的数据：`select * from student_partition where user_address = 'Guangdong'`
4. 多分区联合查询
```sql
select * from student_partition where user_address='JiangXi'
union
select * from student_partition where user_address='Hunan'
union
select * from student_partition where user_address='Guangdong';
```
5. 增加单个分区：`alter table student_partition add partition(user_address='Zhejiang');`
6. 增加多个分区：`alter table student_partition add partition(user_address='Zhejiang'), partition(user_address='Shanghai');`
7. 删除多个分区：`alter table student_partition drop partition (user_address='Zhejiang'), partition(user_address='Shanghai');` 


#### 修改删除表
1. 重命名表：`ALTER TABLE table_name RENAME TO new_table_name`
2. 更新列：`ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]`
3. 增加替换列：`ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) `
4. 删除表：`drop table student;`


#### 导入数据到表
1. 加载txt到表中：`load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];`
2. 通过查询语句向表插入数据：`insert into table student partition(month='201709') values(1,'wangwu'),(2,’zhaoliu’);`
3. 根据查询结果创建表：`create table if not exists student3 as select id, name from student;`
4. 建表时location指定加载数据路径
```sql
dfs -mkdir /student;
dfs -put /opt/module/datas/student.txt /student;

create external table if not exists student5(
id int, name string
)
row format delimited fields terminated by '\t'
location '/student;
```
5. import数据到表: `import table student2 partition(month='201709') from '/user/hive/warehouse/export/student';`

#### 从数据库导出表
1. 导出到本地：`insert overwrite local directory '/opt/module/datas/export/student' select * from student;`
2. 格式化导出到本地：`insert overwrite local directory '/opt/module/datas/export/student1' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student;`
3. 导出到HDFS：`insert overwrite directory '/student2' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' select * from student`
4. hadoop命令导出：`dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student3.txt;`
5. Hive Shell导出：` bin/hive -e 'select * from default.student;' > /opt/module/datas/export/student4.txt;`

### 表查询操作(基本和MySQL一样)
1. 全表查询：`select * from member;`
2. 条件查询：`select * from member where id = 1`
3. 列别名：`select name as member_name from member;`
4. 常用函数：`count(1) 计数， max(score) 求最大值， min(score) 求最小值， sum(score) 求总值， avg(score) 求平均值`
5. 限制行数：`select * from member limit 100;`
6. 去重查询：`select distinct username from member`
7. like查询：`select * from member where name like '_aa%'`，_表示一个占位，%多个占位。
8. rlike查询：`select * from member where name rlike '[2]'`,rlike通过正则来查询。
9. 分组查询：`select * from member m group by m.id`
10. order，sort排序查询：`order by时有单独reducer对所有数据进行排序，数据是全局有序的，sort反之。`
11. having查询：`select username, sum(score) from member group by username having sum(score) > 540`,仅能用于group by分组统计。
12. 满外连接：`select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;`