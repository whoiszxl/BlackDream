### Sqoop简介

Sqoop可以将传统数据库如MySQL ,Oracle等的数据导入到HDFS、Hive、HBase 等分布式文件存储系统中，反之亦可。

### Sqoop安装
> 推荐用1.x版本的，不推荐用2.x版本的

1. 下载包：`wget http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.15.2.tar.gz`
2. 解压：`tar -zxvf sqoop-1.4.6-cdh5.15.2.tar.gz -C ../module/ && cd /opt/module/ && mv sqoop-1.4.6-cdh5.15.2/ sqoop`
3. 修改配置文件：`cd /opt/module/sqoop/conf/ && cp sqoop-env-template.sh sqoop-env.sh && vi sqoop-env.sh`
```sh
# Set Hadoop-specific environment variables here.
#Set path to where bin/hadoop is available
export HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2

#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2

#set the path to where bin/hbase is available
export HBASE_HOME=/opt/module/hbase

#Set the path to where bin/hive is available
export HIVE_HOME=/opt/module/hive

#Set the path for where zookeper config dir is
export ZOOCFGDIR=/opt/module/zookeeper
export ZOOKEEPER_HOME=/opt/module/zookeeper
```
4. 拷贝文件`mysql-connector-java-5.1.27-bin.jar`到lib目录下
5. 测试是否配置成功：`bin/sqoop help`
6. 测试是否能连接数据库：`bin/sqoop list-databases --connect jdbc:mysql://rm-wz9p7ne0m7sqb751.mysql.rds.aliyuncs.com:3306/ --username hive --password hive`
7. 查询数据库中的表：`bin/sqoop list-tables --connect jdbc:mysql://rm-wz9p7ne0m7sqb751.mysql.rds.aliyuncs.com:3306/hive --username hive --password hive1020`



### 导入数据
> 概念：从传统关系型数据库导入到大数据集群方案中

#### 0. 导入的时候报错
错误：`Exception in thread "main" java.lang.NoClassDefFoundError: org/json/JSONObject`
解决方法：下载`http://www.java2s.com/Code/Jar/j/Downloadjavajsonjar.htm`中的包解压到lib下

#### 1. 从MySQL导入到HDFS
```shell
#全表导入
bin/sqoop import \
--connect jdbc:mysql://rm-wz9p7ne0m7sqb751.mysql.rds.aliyuncs.com:3306/hive \
--username hive \
--password hive1020 \
--table member \
--target-dir /user/member \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t"

#查询导入
bin/sqoop import \
--connect jdbc:mysql://rm-wz9p7ne0m7sqb751.mysql.rds.aliyuncs.com:3306/hive \
--username hive \
--password hive1020 \
--target-dir /user/member \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t" \
--query 'select username,password from member where username = "zhangsan" and $CONDITIONS;'

#导入指定列,列之间不需要空格
bin/sqoop import \
--connect jdbc:mysql://hadoop001:3306/demo \
--username root \
--password root \
--target-dir /user/demo \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t" \
--columns username,password \
--table member

#指定条件导入
bin/sqoop import \
--connect jdbc:mysql://hadoop001:3306/demo \
--username root \
--password root \
--target-dir /user/demo \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t" \
--table member \
--where "id=1"
```


#### 2. 从MySQL导入到Hive
增加`--incremental append`参数可以增量导入,其要求`--check-column`指定的列值是递增的，从`--last-value`的值开始导入。
```shell
bin/sqoop import \
--connect jdbc:mysql://rm-wz9p7ne0m7sqb751.mysql.rds.aliyuncs.com:3306/hive \
--username hive \
--password hive \
--table member \
--num-mappers 1 \
--hive-import \
--fields-terminated-by "\t" \
--hive-overwrite \
--hive-table xl_user
```

#### 3. 从MySQL导入到Hbase
```shell
bin/sqoop import \
--connect jdbc:mysql://hadoop001:3306/demo \
--username root \
--password root \
--table member \
--columns "username,password" \
--column-family "info" \
--hbase-create-table \
--hbase-row-key "id" \
--hbase-table "hbase_member" \
--num-mappers 1 \
--split-by id
```




### 导出数据

#### 1. HIVE/HDFS到MySQL
```shell
bin/sqoop export \
--connect jdbc:mysql://hadoop001:3306/demo \
--username root \
--password root \
--table member \
--num-mappers 1 \
--export-dir /user/hive/warehouse/member_hive \
--input-fields-terminated-by "\t"
```

### 脚本方式从HDFS到MySQL
1. 创建一个脚本文件：`vi job_hdfs2mysql.opt`
```shell
export
--connect
jdbc:mysql://hadoop001:3306/demo
--username
root
--password
root
--table
member
--num-mappers
1
--export-dir
/user/hive/warehouse/member_hive
--input-fields-terminated-by
"\t"
```